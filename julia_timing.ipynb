{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800-element Vector{Float64}:\n",
       " 662.6942976987755\n",
       " 618.4764344469371\n",
       " 612.1582081961544\n",
       " 614.2572157292472\n",
       " 655.1992252045744\n",
       " 615.1134814771395\n",
       " 608.1138685515541\n",
       " 599.6363849927284\n",
       " 670.2068321240985\n",
       " 608.8750027638932\n",
       "   ⋮\n",
       " 625.0953992137233\n",
       " 649.6584266431195\n",
       " 617.2160336720075\n",
       " 594.6136767234705\n",
       " 591.3045195746392\n",
       " 637.7551374911894\n",
       " 621.0331497398638\n",
       " 647.5278389830117\n",
       " 616.18172902362"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "using SparseArrays\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools\n",
    "\n",
    "# Sigmoid function\n",
    "sigmoid(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "# Set random seed\n",
    "rng = MersenneTwister(1)\n",
    "\n",
    "n, d, k = 1_000_000, 10, 800\n",
    "X = randn(rng, n, d)\n",
    "beta = randn(rng, d)\n",
    "g = rand(rng, 1:k, n)\n",
    "\n",
    "# Construct G matrix and Y from the groups g and the data X\n",
    "data = ones(n)\n",
    "indices = hcat(1:n, g)\n",
    "G = sparse(indices[:,1], indices[:,2], data, n, k)'\n",
    "Y = G * sigmoid.(X * beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparse_loss_fn (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function sparse_loss_fn(beta)\n",
    "    return mean((G * sigmoid.(X * beta) .- Y).^2, dims=1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×100 Matrix{Float64}:\n",
       " 237.665  237.665  237.665  237.665  …  237.665  237.665  237.665  237.665"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sparse_loss_fn(params .* 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `gradient` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `gradient` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Dropbox/elrpy/Untitled-1.ipynb:2"
     ]
    }
   ],
   "source": [
    "params = randn(rng, d)\n",
    "grad = gradient(sparse_loss_fn, beta*0.)\n",
    "# hess = hessian(sparse_loss_fn, beta*0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff, ReverseDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hessian_fn (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradient function using ForwardDiff\n",
    "grad_fn(beta) = ForwardDiff.jacobian(b -> sparse_loss_fn(b), beta)\n",
    "# grad_fn(beta) = ReverseDiff.gradient(b -> sparse_loss_fn(b), beta)\n",
    "\n",
    "# Hessian function using Jacobian of the gradient function\n",
    "hessian_fn(beta) = ForwardDiff.jacobian(grad_fn, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = randn(rng, (d, 100))\n",
    "hessian_fn(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = randn(rng, (d, 100))\n",
    "hessian_fn(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 25 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m196.303 ms\u001b[22m\u001b[39m … \u001b[35m219.948 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m1.43% … 6.13%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m206.446 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m6.50%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m207.670 ms\u001b[22m\u001b[39m ± \u001b[32m  5.079 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m6.26% ± 1.05%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m█\u001b[39m \u001b[39m▃\u001b[34m \u001b[39m\u001b[39m▃\u001b[39m█\u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \n",
       "  \u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
       "  196 ms\u001b[90m           Histogram: frequency by time\u001b[39m          220 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m1.59 GiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m80\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = randn(rng, d)\n",
    "@benchmark hessian_fn($params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×10 Matrix{Float64}:\n",
       " -0.602326  -0.0310429   0.682521     …  -1.19938     0.784479  -0.0979281\n",
       "  0.439605   1.98135    -0.564195         2.14695    -0.144549   1.03334\n",
       "  0.214994  -0.370731    0.000905333     -1.19772    -0.553867   0.285461\n",
       " -0.341322   1.05155     1.71632          1.36741     0.144165   0.937683\n",
       "  1.03744    0.431556   -0.107349        -1.60741     1.1509    -0.463932\n",
       "  0.633397  -0.0883078  -0.594694     …  -0.497014    0.180204   0.571343\n",
       " -0.261893  -0.460954    1.05734         -0.525668    0.283683  -1.75742\n",
       " -1.21796   -0.455816    0.452604        -0.0846535  -0.402552  -0.0730914\n",
       "  1.65319    0.328531    0.346625        -0.323957    0.231393  -1.5412\n",
       "  0.710281  -0.589103   -1.14473          0.620849   -0.468667   1.38703\n",
       "  ⋮                                   ⋱                         \n",
       "  0.119559  -0.957195   -0.286772        -0.540889   -0.278937   0.0283729\n",
       "  0.26452   -0.999995    1.88445          0.573313    0.42162    1.40595\n",
       " -2.08181   -0.238089    0.453112        -1.16722    -0.519589   0.486861\n",
       " -1.99136    1.1695     -0.396487        -1.14549     0.47144   -0.477401\n",
       " -0.68252   -0.336572   -0.565343     …  -0.158134    1.31624   -0.451065\n",
       "  0.371057   2.28776    -1.35976          1.10571    -0.949429  -0.172723\n",
       " -1.17309   -0.164913   -0.0596726       -0.54385    -1.04293   -0.431333\n",
       " -1.29389   -0.0393253   0.69792          2.05744     0.79578    0.947569\n",
       "  0.608627   0.651349   -0.267141        -0.668235   -0.145045   0.216358"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = randn(rng, (100, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:\n",
      "\n",
      "Stacktrace:\n",
      "  [1] Array\n",
      "    @ ./boot.jl:479 [inlined]\n",
      "  [2] similar\n",
      "    @ ./array.jl:372 [inlined]\n",
      "  [3] track(x::Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}, ::Type{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}, tp::Vector{ReverseDiff.AbstractInstruction})\n",
      "    @ ReverseDiff ~/.julia/packages/ReverseDiff/UJhiD/src/tracked.jl:473\n",
      "  [4] record_mul\n",
      "    @ ~/.julia/packages/ReverseDiff/UJhiD/src/derivatives/linalg/arithmetic.jl:173 [inlined]\n",
      "  [5] *(x::Matrix{Float64}, y::ReverseDiff.TrackedArray{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}, 2, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}})\n",
      "    @ ReverseDiff ~/.julia/packages/ReverseDiff/UJhiD/src/derivatives/linalg/arithmetic.jl:214\n",
      "  [6] sparse_loss_fn(beta::ReverseDiff.TrackedArray{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}, 2, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}})\n",
      "    @ Main ~/Dropbox/elrpy/Untitled-1.ipynb:2\n",
      "  [7] #13\n",
      "    @ ~/Dropbox/elrpy/Untitled-1.ipynb:3 [inlined]\n",
      "  [8] ReverseDiff.GradientTape(f::var\"#13#14\", input::Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}, cfg::ReverseDiff.GradientConfig{ReverseDiff.TrackedArray{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}, 2, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}}})\n",
      "    @ ReverseDiff ~/.julia/packages/ReverseDiff/UJhiD/src/api/tape.jl:199\n",
      "  [9] gradient(f::Function, input::Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}, cfg::ReverseDiff.GradientConfig{ReverseDiff.TrackedArray{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}, ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}, 2, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}}})\n",
      "    @ ReverseDiff ~/.julia/packages/ReverseDiff/UJhiD/src/api/gradients.jl:22\n",
      " [10] gradient\n",
      "    @ ~/.julia/packages/ReverseDiff/UJhiD/src/api/gradients.jl:22 [inlined]\n",
      " [11] grad_fn\n",
      "    @ ~/Dropbox/elrpy/Untitled-1.ipynb:3 [inlined]\n",
      " [12] chunk_mode_jacobian(f::typeof(grad_fn), x::Matrix{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}})\n",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/PcZ48/src/jacobian.jl:183\n",
      " [13] jacobian(f::Function, x::Matrix{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}}, ::Val{true})\n",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/PcZ48/src/jacobian.jl:0\n",
      " [14] jacobian(f::Function, x::Matrix{Float64}, cfg::ForwardDiff.JacobianConfig{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{typeof(grad_fn), Float64}, Float64, 12}}})\n",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/PcZ48/src/jacobian.jl:19\n",
      " [15] jacobian(f::Function, x::Matrix{Float64})\n",
      "    @ ForwardDiff ~/.julia/packages/ForwardDiff/PcZ48/src/jacobian.jl:19\n",
      " [16] hessian_fn(beta::Matrix{Float64})\n",
      "    @ Main ~/Dropbox/elrpy/Untitled-1.ipynb:6\n",
      " [17] eval\n",
      "    @ ./boot.jl:370 [inlined]\n",
      " [18] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1903\n",
      " [19] #invokelatest#2\n",
      "    @ ./essentials.jl:819 [inlined]\n",
      " [20] invokelatest\n",
      "    @ ./essentials.jl:816 [inlined]\n",
      " [21] (::VSCodeServer.var\"#202#203\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.60.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:19\n",
      " [22] withpath(f::VSCodeServer.var\"#202#203\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.60.2/scripts/packages/VSCodeServer/src/repl.jl:274\n",
      " [23] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.60.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:13\n",
      " [24] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.60.2/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [25] serve_notebook(pipename::String, outputchannel_logger::Base.CoreLogging.SimpleLogger; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.60.2/scripts/packages/VSCodeServer/src/serve_notebook.jl:139\n",
      " [26] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.60.2/scripts/notebook/notebook.jl:32"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10×100 Array{Float64, 3}:\n",
       "[:, :, 1] =\n",
       " 36.7245      4.00898    3.70059  …    5.72025  -1.14681    1.6965\n",
       "  4.00898    10.2771    -4.17017     -23.2168   -5.16176   -4.48953\n",
       "  3.70059    -4.17017   32.2055       -4.81301  -9.28884    2.56492\n",
       " -4.63648     0.721872  -1.24823      -1.29031   0.455441   2.0115\n",
       "  2.88783   -13.4894    -8.99396      -7.78452  -3.45405    2.83877\n",
       " -0.545859    4.0106    -2.03703  …   11.9496    4.8398     2.9264\n",
       "  0.210985   -3.52073   -1.22296      -1.59566   0.945897   4.54254\n",
       "  5.72025   -23.2168    -4.81301      17.9613   -5.12465   -2.47633\n",
       " -1.14681    -5.16176   -9.28884      -5.12465  38.0845     2.71252\n",
       "  1.6965     -4.48953    2.56492      -2.47633   2.71252   30.3502\n",
       "\n",
       "[:, :, 2] =\n",
       "  8.08067   -6.30789   9.39886   …    3.20356     0.918254  -1.94107\n",
       " -6.30789   10.3493    2.87554       -3.09473     3.02206    1.61251\n",
       "  9.39886    2.87554   5.23886       -0.256905   -5.36278   -5.89122\n",
       "  6.90496    2.78602  -7.28283       -4.38843    -4.23962   -4.60154\n",
       " -8.7981     7.03111  -3.56292        1.0263      3.60872   -5.10962\n",
       " -2.07536    6.58134  -0.59737   …  -14.1282     -6.84597    5.33468\n",
       "  8.332      3.56586  -6.25973       -5.44745   -26.448     -8.18121\n",
       "  3.20356   -3.09473  -0.256905      15.2188     -8.79411   -1.24928\n",
       "  0.918254   3.02206  -5.36278       -8.79411    -3.942      7.49216\n",
       " -1.94107    1.61251  -5.89122       -1.24928     7.49216   22.8039\n",
       "\n",
       "[:, :, 3] =\n",
       "  16.7191     0.792338   4.84777   …  -4.12074    5.3765   -10.3153\n",
       "   0.792338   9.14661    1.49161      -5.19488   -2.59046    1.05161\n",
       "   4.84777    1.49161    6.66116       4.58449   -1.52471    0.574375\n",
       "   2.1472     3.16655   -5.84861      -4.80915    8.12249    1.55542\n",
       "   2.48191   -2.62166   -5.07774      -0.551032  11.3284     2.55684\n",
       "   5.16434   -5.00934   -1.53322   …  13.9045    -9.6223     0.628676\n",
       "  -1.4704    -2.13525    4.10761      -4.92019    7.92424    4.16941\n",
       "  -4.12074   -5.19488    4.58449       4.00427    4.33583    2.08601\n",
       "   5.3765    -2.59046   -1.52471       4.33583    4.28518    6.39663\n",
       " -10.3153     1.05161    0.574375      2.08601    6.39663    5.53837\n",
       "\n",
       ";;; … \n",
       "\n",
       "[:, :, 98] =\n",
       " -0.702449   2.85453     0.628806  …   3.41263     0.218172   0.305133\n",
       "  2.85453   -4.50029    -1.44089      -3.9511      1.32452    1.79341\n",
       "  0.628806  -1.44089    -6.78686       5.82793    -5.89982   -1.76906\n",
       " -2.56236    2.91566     5.3704       -0.407816    1.21418   -1.3449\n",
       " -4.7902    -0.0219858  -0.620738     -4.29939    13.6662    11.182\n",
       "  3.81186    0.969262    0.59544   …  -0.0785436   3.3292    -0.466321\n",
       "  3.1278     2.02476    -3.33607       0.0330883  -8.92418   -4.33294\n",
       "  3.41263   -3.9511      5.82793       2.91096    -1.08896   -0.108356\n",
       "  0.218172   1.32452    -5.89982      -1.08896    -2.02631    0.579867\n",
       "  0.305133   1.79341    -1.76906      -0.108356    0.579867  -9.26455\n",
       "\n",
       "[:, :, 99] =\n",
       " 12.8327    -3.21263   4.1661     …  -0.40438   -8.86378     -5.67844\n",
       " -3.21263    8.42713  -2.11592       -1.78409   -1.7564      -7.27485\n",
       "  4.1661    -2.11592   8.36387       -2.63318    0.0417012   -1.25669\n",
       " -5.42086    3.49133   1.27101       -3.85867   -2.41609     -3.90448\n",
       "  0.923494   4.28037  -2.39847        0.349883   0.267519    -1.77304\n",
       " -1.84456   -4.91842  -1.85887    …  -1.11672    7.57485     -3.73886\n",
       " -3.9714     4.63926   1.99396        2.68437    0.609729    -2.46995\n",
       " -0.40438   -1.78409  -2.63318       13.93      -1.34232      0.816805\n",
       " -8.86378   -1.7564    0.0417012     -1.34232    8.80705     -6.78015\n",
       " -5.67844   -7.27485  -1.25669        0.816805  -6.78015    -11.2379\n",
       "\n",
       "[:, :, 100] =\n",
       " 38.9147      5.5104   -0.902723  -7.51639   …  -4.78643  -6.85067   1.10788\n",
       "  5.5104     22.902    -5.43182    1.04785       7.49636  11.2      -1.90537\n",
       " -0.902723   -5.43182  38.2295     2.11095       4.94276  -2.75727  -1.47558\n",
       " -7.51639     1.04785   2.11095   41.3954       -1.9438    1.7475   -0.920823\n",
       "  1.95792    -5.06067  -4.986      1.45532       1.46303  -4.34371   2.08855\n",
       " -0.426293    3.66569  -3.24745    2.23257   …  -3.78284   6.90448   1.59455\n",
       " 15.649     -19.3689   -4.65403    8.20121      16.7491   10.4427    9.09718\n",
       " -4.78643     7.49636   4.94276   -1.9438       37.114    -6.43116   2.34415\n",
       " -6.85067    11.2      -2.75727    1.7475       -6.43116  45.0599    7.91601\n",
       "  1.10788    -1.90537  -1.47558   -0.920823      2.34415   7.91601  30.5024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combinedims(map(hessian_fn, splitdims(params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Vector{Matrix{Float64}}:\n",
       " [16.523745630824568 -4.6953282538450765 … 1.5638174070204032 -2.095916997853869; -4.695328253845178 8.657300043715523 … 12.590106455442822 1.0923772048364302; … ; 1.5638174070204314 12.590106455442005 … -1.1493041090198277 -5.804361016964054; -2.0959169978540433 1.0923772048364013 … -5.804361016964088 11.82033818306524]\n",
       " [13.056847654459544 -2.3024338254102 … -1.9466051069024148 9.376021838983707; -2.302433825410207 16.66279211425452 … -0.15461548723436305 3.529231998054975; … ; -1.9466051069023271 -0.1546154872343323 … 28.913395497825324 6.836305379958778; 9.376021838983924 3.5292319980551077 … 6.836305379959158 -5.034997877820383]\n",
       " [7.757946949836845 -1.1679881753242054 … -9.572152334085358 -6.554612563630759; -1.1679881753242491 6.238703519512129 … -6.115279873801038 0.71984400994688; … ; -9.572152334085386 -6.115279873801363 … -1.7520807009998884 2.598284358639915; -6.554612563631305 0.7198440099469344 … 2.5982843586399036 5.296856544609978]\n",
       " [16.923357015881283 -10.887595769023575 … 4.146446458662009 -5.625967832060255; -10.88759576902377 8.418715209652973 … 3.724593534002626 -8.245589494547591; … ; 4.146446458662085 3.724593534002698 … 24.622083067033156 7.212615520007285; -5.625967832060278 -8.245589494547746 … 7.212615520007707 21.63857607438551]\n",
       " [-2.976915819099508 0.22124156735856418 … 2.7818117903579895 -0.8689235739639107; 0.22124156735834813 -3.933400242523349 … 6.966550087382677 3.7424179396511534; … ; 2.7818117903580823 6.966550087382544 … -9.866058773051705 -1.573315544513098; -0.8689235739639327 3.742417939650995 … -1.5733155445130573 8.26958315120057]\n",
       " [1.947849670700848 2.7552715017917144 … 10.58879995374167 10.381789185035515; 2.7552715017913427 14.697485505194019 … -2.2934369814124387 -12.27855184118181; … ; 10.588799953741123 -2.2934369814125377 … 13.82084573180175 4.237732672721364; 10.381789185035446 -12.278551841182972 … 4.2377326727212905 3.848098896623046]\n",
       " [24.7791812346324 2.4617123925583195 … 1.6171016361432538 -0.42839837916867535; 2.461712392558202 17.731242811690596 … -13.846904753328115 -4.709222227537966; … ; 1.6171016361430202 -13.84690475332802 … 9.783696515541301 -8.05127404302665; -0.42839837916876844 -4.709222227537848 … -8.05127404302616 -1.118097335825465]\n",
       " [38.10913653808858 3.834837377066141 … 14.972840932074917 -10.887814923304296; 3.83483737706621 41.64436758394668 … 5.235946923488566 2.2187118873014096; … ; 14.972840932074625 5.235946923488868 … 41.268381569375784 13.947795578329787; -10.887814923304862 2.218711887301299 … 13.947795578330721 27.32248699271821]\n",
       " [10.123482409994338 1.639729335330363 … -13.028296235364646 -8.312610911934781; 1.6397293353301583 -2.7396645013866667 … -1.9010605331895178 6.561233871814968; … ; -13.028296235364197 -1.901060533189547 … -29.0874246810732 -15.13483944607896; -8.312610911935682 6.561233871815108 … -15.134839446077365 -16.948015487084216]\n",
       " [-6.9492997399997085 0.04937340297915674 … 14.150085968378015 4.83323391946439; 0.049373402979230534 -0.08107758309761158 … -1.272217542141747 -1.3470586822567487; … ; 14.150085968377642 -1.2722175421417825 … 7.657889083308825 -1.5334522211184571; 4.833233919464539 -1.3470586822566046 … -1.5334522211183934 2.474277616572548]\n",
       " ⋮\n",
       " [27.301386724091746 1.905533361023329 … -3.9987018752939214 -0.5544164632871665; 1.9055333610232281 13.71908096250288 … 1.0499061319932461 1.6579191943998; … ; -3.998701875293696 1.0499061319931946 … 13.145414418701854 -5.616849314287687; -0.554416463287218 1.6579191943998288 … -5.616849314287196 -4.598423920408126]\n",
       " [10.786322861327415 -0.09105472106569443 … -1.1399902152393757 -2.8202873507846347; -0.09105472106573094 8.978058234008849 … -2.40313180306605 -5.57566090249873; … ; -1.1399902152393682 -2.403131803066125 … 1.8954468717898434 -3.43519697311428; -2.820287350784796 -5.575660902498825 … -3.435196973114208 0.280473119742315]\n",
       " [3.9200306529118154 9.076218488808346 … 8.946465514480561 4.571820957544816; 9.076218488807745 13.327008749656619 … -1.2731672439094153 -10.886831932760602; … ; 8.946465514480234 -1.2731672439096027 … 4.38084016640482 11.235556135265107; 4.571820957544885 -10.886831932760364 … 11.235556135265172 16.75657815300714]\n",
       " [16.61422532835387 2.47242691495344 … -16.268500456351962 -9.424415075392922; 2.472426914953395 4.4425190360150975 … -6.064429700674911 8.305905951803803; … ; -16.268500456351948 -6.0644297006749195 … -2.9007286857402086 -1.4768381942476754; -9.424415075393386 8.305905951804068 … -1.476838194247809 0.860828691860557]\n",
       " [38.328121634695314 2.7580681060909833 … -0.4570268605847547 -3.1134311841857345; 2.7580681060910375 35.2980582613106 … 4.89553169136205 -3.1298511571428573; … ; -0.45702686058475445 4.8955316913615805 … 33.33816642792878 3.990719633474996; -3.1134311841856412 -3.129851157143014 … 3.990719633474774 35.06902683162036]\n",
       " [-2.3777144917346718 -10.033803887527183 … -0.7740318854062228 -12.533347987788447; -10.0338038875269 -12.248741650571553 … 5.576452851266985 2.6879100870588664; … ; -0.7740318854062601 5.57645285126677 … -23.262101425252286 -4.348779864360407; -12.533347987788833 2.6879100870585853 … -4.348779864360379 1.7329302503599486]\n",
       " [28.279173334568213 -1.8672679335653235 … -1.534400837994567 -0.976666341315315; -1.8672679335652889 5.416951307003277 … -7.7597362919243125 -2.0172892548723667; … ; -1.534400837994449 -7.759736291924185 … 25.986302164954704 6.755780677941397; -0.9766663413153159 -2.017289254872129 … 6.7557806779416785 22.416876283735267]\n",
       " [23.57638363340586 8.590351853261332 … -17.16559443014356 -3.2611027585531973; 8.590351853261131 12.487844852504159 … 5.988151125493018 11.551156639110488; … ; -17.165594430143734 5.988151125493173 … -10.389891964040011 -21.242923791164998; -3.2611027585531516 11.551156639110982 … -21.242923791164067 -27.219450724442424]\n",
       " [11.867097476443073 -0.011107563046619912 … -1.7399950836205547 0.6539443317868151; -0.011107563046657628 10.208855209774795 … -1.971683106445068 1.378785450788488; … ; -1.739995083620489 -1.9716831064451203 … 9.877574123297212 2.630313327822788; 0.6539443317868028 1.3787854507884754 … 2.630313327822679 12.756384587001314]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hessian_fn.(eachrow(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 1 sample with 1 evaluation.\n",
       " Single result which took \u001b[34m20.992 s\u001b[39m (7.18% GC) to evaluate,\n",
       " with a memory estimate of \u001b[33m158.75 GiB\u001b[39m, over \u001b[33m8308\u001b[39m allocations."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark hessian_fn.(eachrow($params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching sparse_loss_fn(::Vector{Float64}, ::Matrix{Float64}, ::Vector{Float64}, ::Adjoint{Float64, SparseMatrixCSC{Float64, Int64}})\n\nClosest candidates are:\n  sparse_loss_fn(::Any)\n   @ Main ~/Dropbox/elrpy/Untitled-1.ipynb:1\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching sparse_loss_fn(::Vector{Float64}, ::Matrix{Float64}, ::Vector{Float64}, ::Adjoint{Float64, SparseMatrixCSC{Float64, Int64}})\n",
      "\n",
      "Closest candidates are:\n",
      "  sparse_loss_fn(::Any)\n",
      "   @ Main ~/Dropbox/elrpy/Untitled-1.ipynb:1\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] macro expansion\n",
      "   @ ~/.julia/packages/Zygote/YYT6v/src/compiler/interface2.jl:101 [inlined]\n",
      " [2] _pullback(::Zygote.Context{false}, ::typeof(sparse_loss_fn), ::Vector{Float64}, ::Matrix{Float64}, ::Vector{Float64}, ::Adjoint{Float64, SparseMatrixCSC{Float64, Int64}})\n",
      "   @ Zygote ~/.julia/packages/Zygote/YYT6v/src/compiler/interface2.jl:101\n",
      " [3] pullback(::Function, ::Zygote.Context{false}, ::Vector{Float64}, ::Vararg{Any})\n",
      "   @ Zygote ~/.julia/packages/Zygote/YYT6v/src/compiler/interface.jl:44\n",
      " [4] pullback(::Function, ::Vector{Float64}, ::Matrix{Float64}, ::Vararg{Any})\n",
      "   @ Zygote ~/.julia/packages/Zygote/YYT6v/src/compiler/interface.jl:42\n",
      " [5] gradient(::Function, ::Vector{Float64}, ::Vararg{Any})\n",
      "   @ Zygote ~/.julia/packages/Zygote/YYT6v/src/compiler/interface.jl:96\n",
      " [6] top-level scope\n",
      "   @ ~/Dropbox/elrpy/Untitled-1.ipynb:2"
     ]
    }
   ],
   "source": [
    "# Gradient and Hessian functions\n",
    "sparse_grad = gradient(sparse_loss_fn, beta, X, Y, G)\n",
    "sparse_hess = hessian(sparse_loss_fn, beta, X, Y, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
